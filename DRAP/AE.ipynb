{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGPbsbYsIBWx"
   },
   "source": [
    "# Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3628,
     "status": "ok",
     "timestamp": 1734341417823,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "1k1acISNIFaE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path1, file_path2, file_path3, file_path4):\n",
    "        \"\"\"\n",
    "        Dataset class for the model\n",
    "\n",
    "        Args:\n",
    "            data_paths (str): Path to .pt files\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        p1 = torch.load(file_path1)\n",
    "        p2 = torch.load(file_path2)\n",
    "        p3 = torch.load(file_path3)\n",
    "        p4 = torch.load(file_path4)\n",
    "\n",
    "        self.data = p1 + p2 + p3 + p4\n",
    "\n",
    "        del p1, p2, p3, p4\n",
    "        gc.collect()\n",
    "\n",
    "        self.pad = nn.ReplicationPad2d(4)\n",
    "        self.crop_size = 84\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - \"seq_canonical\" (torch.Tensor): Concatenated canonical sequence of shape [9, H, W],\n",
    "                where the temporal dimension (3 frames) is merged into the channel dimension (3 channels * 3 frames = 9).\n",
    "                - \"seq_random\" (torch.Tensor): Concatenated randomized sequence of shape [9, H, W],\n",
    "                where the temporal dimension (3 frames) is merged into the channel dimension.\n",
    "                - \"action\" (torch.Tensor): Action corresponding to the last frame of the sequence (scalar or vector depending on the environment).\n",
    "                - \"target_canon\" (torch.Tensor): Future canonical sequence including the last two frames of the current sequence\n",
    "                and the predicted future frame, of shape [9, H, W].\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        seq_canonical = sample[\"canonical\"]\n",
    "        t_2 = self._random_crop(self.pad(torch.from_numpy(seq_canonical[0]).permute(2,0,1).float()))\n",
    "        t_1 = self._random_crop(self.pad(torch.from_numpy(seq_canonical[1]).permute(2,0,1).float()))\n",
    "        t_0 = self._random_crop(self.pad(torch.from_numpy(seq_canonical[2]).permute(2,0,1).float()))\n",
    "        seq_canonical = torch.cat([t_2,t_1,t_0], dim=0)\n",
    "\n",
    "        seq_random = sample[\"randomized\"]\n",
    "        seq_random = torch.cat([\n",
    "              self._random_crop(self.pad(torch.from_numpy(seq_random[0]).permute(2,0,1).float())),\n",
    "              self._random_crop(self.pad(torch.from_numpy(seq_random[1]).permute(2,0,1).float())),\n",
    "              self._random_crop(self.pad(torch.from_numpy(seq_random[2]).permute(2,0,1).float()))\n",
    "          ], dim=0)\n",
    "\n",
    "        action_0 = torch.tensor(sample[\"actions\"][2], dtype=torch.float32)\n",
    "\n",
    "        target_canon = sample[\"future_canon\"]\n",
    "        seq_next_canon = torch.cat([\n",
    "            t_1,\n",
    "            t_0,\n",
    "            self._random_crop(self.pad(torch.from_numpy(target_canon).permute(2,0,1).float()))\n",
    "\n",
    "        ], dim=0)\n",
    "\n",
    "        return {\n",
    "            \"seq_canonical\": seq_canonical,   # [9 x H x W]\n",
    "            \"seq_random\": seq_random,         # [9 x H x W]\n",
    "            \"action\": action_0,               # [action_dim]\n",
    "            \"target_canon\": seq_next_canon    # [9 x H x W]\n",
    "        }\n",
    "\n",
    "    def _random_crop(self, padded):\n",
    "        \"\"\"\n",
    "        Performs a random crop on a tensor with padding.\n",
    "\n",
    "        Args:\n",
    "            padded (torch.Tensor): Tensor with padding.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Cropped tensor with the desired size.\n",
    "        \"\"\"\n",
    "        _, channels, height, width = padded.unsqueeze(0).shape  # Aggiunge dimensione batch\n",
    "        crop_x = torch.randint(0, height - self.crop_size + 1, (1,)).item()\n",
    "        crop_y = torch.randint(0, width - self.crop_size + 1, (1,)).item()\n",
    "\n",
    "        cropped = padded[:, crop_x:crop_x + self.crop_size, crop_y:crop_y + self.crop_size]\n",
    "        return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1734341419658,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "UFmJBViaIJHz"
   },
   "outputs": [],
   "source": [
    "def create_dataloader(file_path1, file_path2, file_path3, file_path4, batch_size=128, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for the model.\n",
    "\n",
    "    Args:\n",
    "        file_path1 (str): Path to the first .pt file containing data.\n",
    "        file_path2 (str): Path to the second .pt file containing data.\n",
    "        file_path3 (str): Path to the third .pt file containing data.\n",
    "        file_path4 (str): Path to the fourth .pt file containing data.\n",
    "        batch_size (int): Size of each batch.\n",
    "        shuffle (bool): Whether to shuffle the data.\n",
    "        num_workers (int): Number of parallel processes for data loading.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A PyTorch DataLoader instance.\n",
    "    \"\"\"\n",
    "    dataset = CustomDataset(file_path1, file_path2, file_path3, file_path4)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96589,
     "status": "ok",
     "timestamp": 1734341516237,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "hAHuh19WIZBj",
    "outputId": "df6d0514-413e-4bef-b34b-17a679eb79b4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#Path to store file on Google Drive\n",
    "#output_dir = \"/content/drive/My Drive/Reinforcement Learning/Reinforcement Learning/DRAP\"\n",
    "output_dir = \"/content/drive/My Drive/Reinforcement Learning/DRAP\"\n",
    "\n",
    "# Carica il file .pt\n",
    "#file_path1 = output_dir + \"/dataset/walker/dataset_sequences_1.pt\"\n",
    "#file_path2 = output_dir + \"/dataset/walker/dataset_sequences_2.pt\"\n",
    "#file_path3 = output_dir + \"/dataset/walker/dataset_sequences_3.pt\"\n",
    "#file_path4 = output_dir + \"/dataset/walker/dataset_sequences_4.pt\"\n",
    "file_path1 = output_dir + \"/dataset/walker/dataset_sequences_1.pt\"\n",
    "file_path2 = output_dir + \"/dataset/walker/dataset_sequences_2.pt\"\n",
    "file_path3 = output_dir + \"/dataset/walker/dataset_sequences_3.pt\"\n",
    "file_path4 = output_dir + \"/dataset/walker/dataset_sequences_4.pt\"\n",
    "\n",
    "dataloader = create_dataloader(file_path1, file_path2, file_path3, file_path4, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1734338817459,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "Z3M5RwBwEztx",
    "outputId": "0d11c4ea-f149-4936-fe1c-ac2a1665871b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3257,
     "status": "error",
     "timestamp": 1734338820701,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "WthNc2hwKD5T",
    "outputId": "6ad16cad-00e4-4ff2-bd87-c8efcbaaeaa9"
   },
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "      seq_canonical = batch[\"seq_canonical\"]\n",
    "      seq_random = batch[\"seq_random\"]\n",
    "      action = batch[\"action\"]\n",
    "      seq_next_canon = batch[\"target_canon\"]\n",
    "\n",
    "      print(f\" Sequences lenght: {seq_canonical.shape}, {seq_random.shape}. Action shape: {action.shape}. Target canon shape: {seq_next_canon.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAFOfYeNIaDx"
   },
   "source": [
    "# Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1734341516237,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "s_Mg5BRpyAZd"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734341516237,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "wIMk1wHpuZPW"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Convolutional encoder for image-based observations. Same ad DrQ\"\"\"\n",
    "    def __init__(self, obs_shape, n_features, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_features = n_features\n",
    "        self.img_channels = obs_shape[0]\n",
    "        self.n_filters = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.img_channels, self.n_filters, 3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(self.n_filters, self.n_filters, 3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(self.n_filters, self.n_filters, 3, stride=1)\n",
    "        self.conv4 = nn.Conv2d(self.n_filters, self.n_filters, 3, stride=1)\n",
    "\n",
    "        self.fc = nn.Linear(35 * 35 * self.n_filters, self.n_features)\n",
    "        self.norm = nn.LayerNorm(self.n_features)\n",
    "\n",
    "    def forward(self, obs, detach=False):\n",
    "        obs = obs / 255.0  \n",
    "        self.conv1_output = F.relu(self.conv1(obs))\n",
    "        self.conv2_output = F.relu(self.conv2(self.conv1_output))\n",
    "        self.conv3_output = F.relu(self.conv3(self.conv2_output))\n",
    "        self.conv4_output = F.relu(self.conv4(self.conv3_output))\n",
    "\n",
    "        x = self.conv4_output.reshape(self.conv4_output.size(0), -1)\n",
    "\n",
    "        if detach:\n",
    "            x = x.detach()\n",
    "\n",
    "        self.fc_output = self.fc(x)\n",
    "        self.norm_output = self.norm(self.fc_output)\n",
    "\n",
    "        out = torch.tanh(self.norm_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734341516238,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "2x38jcWisU0n"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder ro reconstruct the images from the embedding.\"\"\"\n",
    "    def __init__(self, n_features, img_channels, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_features = n_features\n",
    "        self.img_channels = img_channels\n",
    "        self.n_filters = 32\n",
    "\n",
    "        #Transform the embedding in a spatial tensor\n",
    "        self.fc1 = nn.Linear(n_features, self.n_filters)\n",
    "        self.fc2 = nn.Linear(self.n_filters, 42 * 42 * self.n_filters)\n",
    "\n",
    "        # Upconvolution\n",
    "        self.upconv = nn.ConvTranspose2d(self.n_filters, self.n_filters, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        #Convolutions\n",
    "        self.conv1 = nn.Conv2d(self.n_filters, self.n_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.n_filters, img_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc1(z))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(f\"post fcs: {x.shape}\")\n",
    "        x = x.view(-1, self.n_filters, 42, 42)  \n",
    "        #print(f\"post view: {x.shape}\")\n",
    "        x = F.relu(self.upconv(x))\n",
    "        #print(f\"post upconv: {x.shape}\")\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(f\"post conv1: {x.shape}\")\n",
    "        x = torch.sigmoid(self.conv2(x)) \n",
    "        #print(f\"post conv2: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734341516238,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "NxJl0u5KzUWe"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP to combine embedding and action.\n",
    "    It predicts the future embedding conditioned by the action.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features + action_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, n_features)\n",
    "\n",
    "    def forward(self, z, action):\n",
    "        #print(z.shape)\n",
    "        #print(action.shape)\n",
    "        x = torch.cat([z, action], dim=-1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734341516238,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "GWXlNpdiubPy"
   },
   "outputs": [],
   "source": [
    "class DRAPModel(nn.Module):\n",
    "    \"\"\"Domain Randomization Removal Pre-training (DRAP) Model.\"\"\"\n",
    "    def __init__(self, obs_shape, n_features, action_dim, device = torch.device(\"cuda\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(obs_shape, n_features, device).to(self.device)\n",
    "        self.mlp = MLP(n_features, action_dim).to(self.device)\n",
    "        self.decoder = Decoder(n_features, obs_shape[0], device).to(self.device)\n",
    "\n",
    "    def forward(self, obs_stack, action):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                obs_stack: Stack of 3 randomized observations (shape: [batch_size, 9, H, W]).\n",
    "                action: lat action t0 (shape: [batch_size, action_dim]).\n",
    "            Returns:\n",
    "                recon_curr: Reconstruction of the sequence (t-2, t-1, t0).\n",
    "                recon_next: Reconstructio of future sequence (t-1, t0, t+1).\n",
    "            \"\"\"\n",
    "            z = self.encoder(obs_stack)  # Bottleneck embedding\n",
    "            #print(f\"z= {z.shape}\")\n",
    "\n",
    "            recon_stack = self.decoder(z)\n",
    "            #print(f\"recon_stack= {recon_stack.shape}\")\n",
    "\n",
    "            z_future = self.mlp(z, action)\n",
    "            #print(f\"z_future= {z_future.shape}\")\n",
    "\n",
    "            recon_next = self.decoder(z_future)\n",
    "\n",
    "            return recon_stack, recon_next\n",
    "\n",
    "    def _train(self, dataloader, epochs=1, lr=3e-4):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            with tqdm(total=len(dataloader), desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "              for batch in dataloader:\n",
    "                  seq_canonical = batch[\"seq_canonical\"]\n",
    "                  seq_random = batch[\"seq_random\"]\n",
    "                  action = batch[\"action\"]\n",
    "                  seq_next_canon = batch[\"target_canon\"]\n",
    "\n",
    "                  seq_random = seq_random.to(self.device)\n",
    "                  seq_canonical = seq_canonical.to(self.device)\n",
    "                  action = action.to(self.device)\n",
    "                  seq_next_canon = seq_next_canon.to(self.device)\n",
    "\n",
    "                  optimizer.zero_grad()\n",
    "                  recon_stack, recon_next = self.forward(seq_random, action)\n",
    "                  loss = criterion(recon_stack, seq_canonical) + criterion(recon_next, seq_next_canon)\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "                  train_loss += loss.item()\n",
    "\n",
    "                  pbar.set_postfix(loss=loss.item())\n",
    "                  pbar.update(1)\n",
    "\n",
    "            avg_train_loss = train_loss / len(dataloader.dataset)\n",
    "            if epoch % 1 == 0:\n",
    "             print(f\"Epoch {epoch + 1}/{epochs}: Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    def save_encoder_weights(self, file_path):\n",
    "        \"\"\"\n",
    "       Save encoder parameters on a .pt file.\n",
    "\n",
    "        Args:\n",
    "            model: DRAP model.\n",
    "            file_path: Path to save the .pt file.\n",
    "        \"\"\"\n",
    "        encoder_weights = self.encoder.state_dict()\n",
    "\n",
    "        torch.save(encoder_weights, file_path)\n",
    "        print(f'Encoder parameters saved in {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121527,
     "status": "ok",
     "timestamp": 1734341646786,
     "user": {
      "displayName": "vincenzo crisa",
      "userId": "15998951029181202074"
     },
     "user_tz": -60
    },
    "id": "gzeQ2-z3TfKk",
    "outputId": "3806412d-54ac-4407-ac4e-0e0e57e5aaa9"
   },
   "outputs": [],
   "source": [
    "model = DRAPModel((9, 84, 88),50,6, device='cuda')\n",
    "model._train(dataloader,1)\n",
    "#model.save_encoder_weights(output_dir + \"/encoder_weights_finger.pt\")\n",
    "model.save_encoder_weights(output_dir + \"/encoder_weights_walker.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
